{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_key = \"AZURE_SPEECH_KEY\"\n",
    "speech_endpoint = \"AZURE_SPEECH_ENDPOINT\"\n",
    "speech_region = \"AZURE_SPEECH_REGION\"\n",
    "\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherfilename=\"en-us_zh-cn.wav\"\n",
    "\n",
    "# set up translation parameters: source language and target languages\n",
    "# Currently the v2 endpoint is required. In a future SDK release you won't need to set it. \n",
    "endpoint_string = \"wss://{}.stt.speech.microsoft.com/speech/universal/v2\".format(speech_region)\n",
    "translation_config = speechsdk.translation.SpeechTranslationConfig(\n",
    "    subscription=speech_key,\n",
    "    endpoint=endpoint_string,\n",
    "    speech_recognition_language='en-US',\n",
    "    target_languages=('de', 'fr'))\n",
    "audio_config = speechsdk.audio.AudioConfig(filename=weatherfilename)\n",
    "\n",
    "# Specify the AutoDetectSourceLanguageConfig, which defines the number of possible languages\n",
    "auto_detect_source_language_config = speechsdk.languageconfig.AutoDetectSourceLanguageConfig(languages=[\"en-US\", \"de-DE\", \"zh-CN\"])\n",
    "\n",
    "# Creates a translation recognizer using and audio file as input.\n",
    "recognizer = speechsdk.translation.TranslationRecognizer(\n",
    "    translation_config=translation_config, \n",
    "    audio_config=audio_config,\n",
    "    auto_detect_source_language_config=auto_detect_source_language_config)\n",
    "\n",
    "# Starts translation, and returns after a single utterance is recognized. The end of a\n",
    "# single utterance is determined by listening for silence at the end or until a maximum of 15\n",
    "# seconds of audio is processed. The task returns the recognition text as result.\n",
    "# Note: Since recognize_once() returns only a single utterance, it is suitable only for single\n",
    "# shot recognition like command or query.\n",
    "# For long-running multi-utterance recognition, use start_continuous_recognition() instead.\n",
    "result = recognizer.recognize_once()\n",
    "\n",
    "# Check the result\n",
    "if result.reason == speechsdk.ResultReason.TranslatedSpeech:\n",
    "    print(\"\"\"Recognized: {}\n",
    "    German translation: {}\n",
    "    French translation: {}\"\"\".format(\n",
    "        result.text, result.translations['de'], result.translations['fr']))\n",
    "elif result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "    print(\"Recognized: {}\".format(result.text))\n",
    "    detectedSrcLang = result.properties[speechsdk.PropertyId.SpeechServiceConnection_AutoDetectSourceLanguageResult]\n",
    "    print(\"Detected Language: {}\".format(detectedSrcLang))\n",
    "elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "    print(\"No speech could be recognized: {}\".format(result.no_match_details))\n",
    "elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "    print(\"Translation canceled: {}\".format(result.cancellation_details.reason))\n",
    "    if result.cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        print(\"Error details: {}\".format(result.cancellation_details.error_details))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
